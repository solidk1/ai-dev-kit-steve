metadata:
  skill_name: databricks-zerobus-ingest
  version: 0.1.0
  created_at: '2026-02-12T15:16:19.824336'
test_cases:
- id: databricks-zerobus-ingest_001
  inputs:
    prompt: Build a simple Python producer that ingests JSON records into a Delta
      table using Zerobus
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Uses ZerobusSdk from zerobus.sdk.sync
    - Uses RecordType.JSON for JSON serialization
    - Creates a stream with sdk.create_stream()
    - Calls stream.ingest_record() to send data
    - Calls ack.wait_for_ack() to confirm durability
    - Closes stream in a finally block
    expected_patterns:
    - pattern: from zerobus\.sdk
      min_count: 1
    - pattern: create_stream
      min_count: 1
    - pattern: ingest_record
      min_count: 1
    - pattern: wait_for_ack
      min_count: 1
    - pattern: (finally|close)
      min_count: 1
    guidelines:
    - Must import from zerobus.sdk.sync or zerobus.sdk.aio
    - Must include proper stream cleanup with try/finally
    - Must show the full init -> stream -> ingest -> ack -> close pattern
  metadata:
    category: happy_path
    difficulty: easy
- id: databricks-zerobus-ingest_002
  inputs:
    prompt: How do I set up authentication and find the server endpoint for Zerobus
      Ingest?
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Requires a service principal with MODIFY and SELECT on the target table
    - Server endpoint varies by cloud (AWS vs Azure)
    - Uses OAuth client_id and client_secret for authentication
    - Target table must be a Unity Catalog managed Delta table
    expected_patterns:
    - pattern: service.principal
      min_count: 0
    - pattern: client_id|client_secret
      min_count: 1
    - pattern: MODIFY|SELECT
      min_count: 1
    guidelines:
    - Must reference setup and authentication documentation
    - Must mention service principal requirements
    - Must explain endpoint format differences per cloud
  metadata:
    category: setup
    difficulty: easy
- id: databricks-zerobus-ingest_003
  inputs:
    prompt: Create an async Python Zerobus producer with Protobuf serialization
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Uses AsyncZerobusSdk from zerobus.sdk.aio
    - Uses RecordType.PROTOBUF for Protobuf serialization
    - Requires a compiled .proto file matching the target table schema
    - Uses SerializeToString() to convert Protobuf messages to bytes
    expected_patterns:
    - pattern: zerobus\.sdk\.aio
      min_count: 1
    - pattern: RecordType\.PROTOBUF
      min_count: 1
    - pattern: async
      min_count: 1
    - pattern: await
      min_count: 1
    guidelines:
    - Must use async/await patterns consistently
    - Must reference Protobuf schema generation from UC table
    - Must include proper async stream cleanup
  metadata:
    category: happy_path
    difficulty: medium
- id: databricks-zerobus-ingest_004
  inputs:
    prompt: Generate a Protobuf schema from a Unity Catalog table for Zerobus ingestion
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Uses Databricks SDK or REST API to fetch table schema
    - Maps UC column types to Protobuf types
    - Generates a .proto file with correct syntax
    - Requires protoc compilation to produce language stubs
    expected_patterns:
    - pattern: \.proto
      min_count: 1
    - pattern: protoc
      min_count: 1
    - pattern: message
      min_count: 1
    guidelines:
    - Must show the type mapping from UC types to Protobuf types
    - Must include protoc compilation step
    - Must produce valid proto3 syntax
  metadata:
    category: happy_path
    difficulty: medium
- id: databricks-zerobus-ingest_005
  inputs:
    prompt: Implement retry and reconnection logic for a Zerobus producer
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Zerobus provides at-least-once delivery guarantees
    - Implement exponential backoff for retries
    - Reinitialize the stream on connection failures
    - Max throughput is 100 MB/s and 15,000 rows/s per stream
    expected_patterns:
    - pattern: (retry|backoff|reconnect)
      min_count: 1
    - pattern: create_stream
      min_count: 1
    guidelines:
    - Must implement exponential backoff, not fixed-interval retries
    - Must reinitialize stream on disconnection, not just retry the same call
    - Must mention at-least-once semantics and idempotency considerations
  metadata:
    category: error_handling
    difficulty: hard
- id: databricks-zerobus-ingest_006
  inputs:
    prompt: Build a Zerobus Ingest client in Go that sends JSON records
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Uses the Go Zerobus SDK
    - Uses gRPC for transport
    - Implements proper error handling and stream cleanup
    - Uses JSON record type
    expected_patterns:
    - pattern: (go|golang|Go)
      min_count: 1
    - pattern: (grpc|gRPC)
      min_count: 1
    guidelines:
    - Must use idiomatic Go patterns (error handling, defer for cleanup)
    - Must reference the multi-language client documentation
    - Must include proper gRPC connection setup
  metadata:
    category: multi_language
    difficulty: medium
- id: databricks-zerobus-ingest_007
  inputs:
    prompt: What are the limits and constraints of Zerobus Ingest?
  outputs:
    response: ''
    execution_success: true
  expectations:
    expected_facts:
    - Max 100 MB/s per stream
    - Max 15,000 rows/s per stream
    - Single-AZ durability
    - At-least-once delivery semantics
    - Table must be a managed Delta table in Unity Catalog
    - Zerobus does not create or alter tables
    expected_patterns:
    - pattern: 100.?MB
      min_count: 1
    - pattern: 15.?000
      min_count: 1
    guidelines:
    - Must list concrete throughput limits
    - Must mention single-AZ durability caveat
    - Must mention that table management is the user's responsibility
  metadata:
    category: knowledge
    difficulty: easy
- id: grp_20260212_153137
  inputs:
    prompt: Build a simple python producer that ingests JSON records into a Delta
      Table using zerobus.
  outputs:
    response: "## Prerequisites\n\nBefore running this producer, make sure the following\
      \ are in place:\n\n1. **Target Delta table exists** -- Zerobus does not create\
      \ or alter tables. Create your managed Delta table in Unity Catalog first:\n\
      \n   ```sql\n   CREATE TABLE catalog.schema.my_events (\n       event_id   \
      \  STRING,\n       device_name  STRING,\n       temp         INT,\n       humidity\
      \     LONG,\n       event_time   TIMESTAMP\n   );\n   ```\n\n2. **Service principal\
      \ with correct grants** -- You need an OAuth2 service principal with `USE CATALOG`,\
      \ `USE SCHEMA`, `MODIFY`, and `SELECT` on the target table:\n\n   ```sql\n \
      \  GRANT USE CATALOG ON CATALOG my_catalog TO `<service-principal-uuid>`;\n\
      \   GRANT USE SCHEMA ON SCHEMA my_catalog.my_schema TO `<service-principal-uuid>`;\n\
      \   GRANT MODIFY, SELECT ON TABLE my_catalog.my_schema.my_events TO `<service-principal-uuid>`;\n\
      \   ```\n\n3. **Install the Zerobus Ingest SDK** (Python 3.9+):\n\n   ```bash\n\
      \   pip install databricks-zerobus-ingest-sdk\n   ```\n\n4. **Set environment\
      \ variables** for your credentials and endpoint:\n\n   ```bash\n   export ZEROBUS_SERVER_ENDPOINT=\"\
      <workspace-id>.zerobus.<region>.cloud.databricks.com\"\n   export DATABRICKS_WORKSPACE_URL=\"\
      https://<instance>.cloud.databricks.com\"\n   export ZEROBUS_TABLE_NAME=\"my_catalog.my_schema.my_events\"\
      \n   export DATABRICKS_CLIENT_ID=\"<service-principal-client-id>\"\n   export\
      \ DATABRICKS_CLIENT_SECRET=\"<service-principal-client-secret>\"\n   ```\n\n\
      \   The server endpoint format depends on your cloud: for AWS it is `<workspace-id>.zerobus.<region>.cloud.databricks.com`,\
      \ for Azure it is `<workspace-id>.zerobus.<region>.azuredatabricks.net`.\n\n\
      ---\n\n## Complete Python Producer (JSON Serialization)\n\n```python\nimport\
      \ os\nimport time\nimport logging\n\nfrom zerobus.sdk.sync import ZerobusSdk\n\
      from zerobus.sdk.shared import (\n    RecordType,\n    IngestRecordResponse,\n\
      \    StreamConfigurationOptions,\n    TableProperties,\n)\n\nlogging.basicConfig(level=logging.INFO)\n\
      logger = logging.getLogger(__name__)\n\n# ---------------------------------------------------------------------------\n\
      # Configuration -- loaded from environment variables\n# ---------------------------------------------------------------------------\n\
      server_endpoint = os.environ[\"ZEROBUS_SERVER_ENDPOINT\"]\nworkspace_url   =\
      \ os.environ[\"DATABRICKS_WORKSPACE_URL\"]\ntable_name      = os.environ[\"\
      ZEROBUS_TABLE_NAME\"]\nclient_id       = os.environ[\"DATABRICKS_CLIENT_ID\"\
      ]\nclient_secret   = os.environ[\"DATABRICKS_CLIENT_SECRET\"]\n\n# ---------------------------------------------------------------------------\n\
      # 1. Initialize the SDK\n# ---------------------------------------------------------------------------\n\
      sdk = ZerobusSdk(server_endpoint, workspace_url)\n\n# ---------------------------------------------------------------------------\n\
      # 2. Configure the stream for JSON record ingestion\n# ---------------------------------------------------------------------------\n\
      options = StreamConfigurationOptions(record_type=RecordType.JSON)\ntable_props\
      \ = TableProperties(table_name)\n\n# ---------------------------------------------------------------------------\n\
      # 3. Create the stream (authenticates with the service principal)\n# ---------------------------------------------------------------------------\n\
      stream = sdk.create_stream(client_id, client_secret, table_props, options)\n\
      \n# ---------------------------------------------------------------------------\n\
      # 4. Ingest records and handle ACKs, with proper cleanup via try/finally\n#\
      \ ---------------------------------------------------------------------------\n\
      try:\n    records_sent = 0\n    for i in range(100):\n        record = {\n \
      \           \"event_id\": f\"evt-{i}\",\n            \"device_name\": f\"sensor-{i\
      \ % 10}\",\n            \"temp\": 20 + (i % 15),\n            \"humidity\":\
      \ 40 + (i % 30),\n            \"event_time\": int(time.time() * 1_000_000),\
      \  # Unix microsecond timestamp\n        }\n\n        ack = stream.ingest_record(record)\n\
      \        ack.wait_for_ack()  # Block until the record is durably written\n \
      \       records_sent += 1\n\n        if records_sent % 25 == 0:\n          \
      \  logger.info(\"Ingested %d records so far\", records_sent)\n\n    logger.info(\"\
      Successfully ingested %d records into %s\", records_sent, table_name)\n\nfinally:\n\
      \    # Always close the stream to release gRPC resources\n    stream.close()\n\
      \    logger.info(\"Stream closed.\")\n```\n\n---\n\n## How It Works\n\nThe producer\
      \ follows the core Zerobus pattern -- **SDK init, create stream, ingest records,\
      \ handle ACKs, close**:\n\n1. **SDK init** -- `ZerobusSdk(server_endpoint, workspace_url)`\
      \ creates the client that manages the gRPC connection to the Zerobus service.\n\
      \n2. **Create stream** -- `sdk.create_stream(...)` authenticates with your service\
      \ principal credentials and opens a bidirectional gRPC stream to the target\
      \ Delta table. The `StreamConfigurationOptions` is set to `RecordType.JSON`\
      \ so the SDK serializes Python dicts as JSON internally.\n\n3. **Ingest records**\
      \ -- Each call to `stream.ingest_record(record)` sends a Python dictionary whose\
      \ keys must match the target table's column names exactly. Timestamp fields\
      \ must be **Unix integer timestamps** (microseconds for Databricks), not ISO\
      \ strings.\n\n4. **Handle ACKs** -- `ack.wait_for_ack()` blocks until the service\
      \ confirms the record has been durably written. This gives you at-least-once\
      \ delivery guarantees. For higher throughput, you can use an ACK callback instead\
      \ (see the variant below).\n\n5. **Close** -- The `try/finally` block ensures\
      \ `stream.close()` always runs, releasing the gRPC connection even if an exception\
      \ occurs during ingestion.\n\n---\n\n## Variant: Higher Throughput with ACK\
      \ Callbacks\n\nFor production workloads where you want to avoid blocking on\
      \ every single record, register an ACK callback and flush at the end:\n\n```python\n\
      from zerobus.sdk.sync import ZerobusSdk\nfrom zerobus.sdk.shared import (\n\
      \    RecordType,\n    IngestRecordResponse,\n    StreamConfigurationOptions,\n\
      \    TableProperties,\n)\n\ndef on_ack(response: IngestRecordResponse) -> None:\n\
      \    logger.info(\"Durable up to offset: %s\", response.durability_ack_up_to_offset)\n\
      \nsdk = ZerobusSdk(server_endpoint, workspace_url)\n\noptions = StreamConfigurationOptions(\n\
      \    record_type=RecordType.JSON,\n    ack_callback=on_ack,\n)\ntable_props\
      \ = TableProperties(table_name)\n\nstream = sdk.create_stream(client_id, client_secret,\
      \ table_props, options)\n\ntry:\n    for i in range(1000):\n        record =\
      \ {\n            \"event_id\": f\"evt-{i}\",\n            \"device_name\": f\"\
      sensor-{i % 10}\",\n            \"temp\": 20 + (i % 15),\n            \"humidity\"\
      : 40 + (i % 30),\n            \"event_time\": int(time.time() * 1_000_000),\n\
      \        }\n        stream.ingest_record(record)  # Non-blocking; ACKs arrive\
      \ via callback\n\n    stream.flush()  # Ensure all buffered records are sent\
      \ before closing\nfinally:\n    stream.close()\n```\n\nThis approach sends records\
      \ without waiting for individual durability confirmations, which is significantly\
      \ faster. The `on_ack` callback fires asynchronously as the server confirms\
      \ writes, and `stream.flush()` ensures all buffered records are dispatched before\
      \ the stream is closed.\n\n---\n\n## Key Points to Remember\n\n- **Zerobus does\
      \ not create tables.** Your target table must already exist as a managed Delta\
      \ table in Unity Catalog.\n- **Timestamp fields must be Unix integer timestamps**\
      \ (microseconds), not string timestamps.\n- **At-least-once delivery.** Downstream\
      \ consumers should be prepared to handle duplicate records.\n- **Throughput\
      \ limits** are 100 MB/s and 15,000 rows/s per stream. For higher throughput,\
      \ open multiple streams.\n- **Schema must match exactly.** The keys in your\
      \ JSON dict must correspond to the column names in the target table. If the\
      \ table schema changes, update your records accordingly."
    execution_success: false
  expectations: {}
  metadata:
    category: happy_path
    difficulty: medium
    source: grp
    approved_by: derek.witt
    approved_at: '2026-02-12T15:36:00.791750'
    skill_version: null
    fixed_by_commit: null
    fix_description: null
    trace_run_id: null
- id: grp_20260212_153428
  inputs:
    prompt: Show me an ideal protobuf schema for a Zerobus demonstration.
  outputs:
    response: "# Protobuf Schema for a Zerobus Demonstration\n\nGreat question. Below\
      \ is a complete walkthrough for building an ideal Protobuf schema for a Zerobus\
      \ demo, using an **IoT sensor telemetry** scenario. This covers the Delta table\
      \ definition, the `.proto` file, compilation, type mappings, and a working Python\
      \ ingestion example.\n\n---\n\n## 1. Create the Target Delta Table\n\nZerobus\
      \ does **not** create or alter tables -- you must pre-create a managed Delta\
      \ table in Unity Catalog. Here is a demo-ready schema for IoT sensor data:\n\
      \n```sql\nCREATE TABLE catalog.schema.iot_sensor_readings (\n    device_id \
      \      STRING,\n    device_name     STRING,\n    location        STRING,\n \
      \   temperature     DOUBLE,\n    humidity        INT,\n    pressure        FLOAT,\n\
      \    battery_pct     FLOAT,\n    is_active       BOOLEAN,\n    reading_time\
      \    TIMESTAMP,\n    tags            ARRAY<STRING>\n);\n```\n\nGrant your service\
      \ principal access:\n\n```sql\nGRANT USE CATALOG ON CATALOG catalog TO `<service-principal-uuid>`;\n\
      GRANT USE SCHEMA ON SCHEMA catalog.schema TO `<service-principal-uuid>`;\nGRANT\
      \ MODIFY, SELECT ON TABLE catalog.schema.iot_sensor_readings TO `<service-principal-uuid>`;\n\
      ```\n\n---\n\n## 2. The `.proto` File Definition\n\nThis `.proto` maps 1:1 to\
      \ the Delta table above, following the Delta-to-Protobuf type mappings:\n\n\
      ```protobuf\nsyntax = \"proto3\";\n\nmessage IoTSensorReading {\n    string\
      \ device_id       = 1;\n    string device_name     = 2;\n    string location\
      \        = 3;\n    double temperature     = 4;\n    int32  humidity        =\
      \ 5;\n    float  pressure        = 6;\n    float  battery_pct     = 7;\n   \
      \ bool   is_active       = 8;\n    int64  reading_time    = 9;   // TIMESTAMP\
      \ -> int64 epoch microseconds\n    repeated string tags   = 10;  // ARRAY<STRING>\
      \ -> repeated string\n}\n```\n\nSave this as `iot_sensor_reading.proto`.\n\n\
      **Critical note on timestamps:** Zerobus requires timestamp fields as **Unix\
      \ epoch microseconds** (int64), not string timestamps. When populating `reading_time`,\
      \ use `int(time.time() * 1_000_000)` in Python.\n\n---\n\n## 3. Generate the\
      \ `.proto` from a UC Table (Alternative)\n\nIf the table already exists, you\
      \ can auto-generate the `.proto` directly from the Unity Catalog schema instead\
      \ of writing it by hand:\n\n```bash\npython -m zerobus.tools.generate_proto\
      \ \\\n    --uc-endpoint \"https://dbc-a1b2c3d4-e5f6.cloud.databricks.com\" \\\
      \n    --client-id \"$DATABRICKS_CLIENT_ID\" \\\n    --client-secret \"$DATABRICKS_CLIENT_SECRET\"\
      \ \\\n    --table \"catalog.schema.iot_sensor_readings\" \\\n    --output iot_sensor_reading.proto\n\
      ```\n\nThis produces a `.proto` file with a message definition matching the\
      \ table schema exactly. If your table schema ever changes, re-run this command\
      \ and recompile.\n\n---\n\n## 4. Compile the Protobuf\n\nInstall `grpcio-tools`,\
      \ then compile:\n\n```bash\npip install grpcio-tools\n\npython -m grpc_tools.protoc\
      \ \\\n    -I. \\\n    --python_out=. \\\n    iot_sensor_reading.proto\n```\n\
      \nThis generates `iot_sensor_reading_pb2.py`. You can verify it works:\n\n```python\n\
      import iot_sensor_reading_pb2\n\nrecord = iot_sensor_reading_pb2.IoTSensorReading(\n\
      \    device_id=\"dev-001\",\n    device_name=\"weather-station-alpha\",\n  \
      \  location=\"Building 7, Floor 3\",\n    temperature=23.5,\n    humidity=62,\n\
      \    pressure=1013.25,\n    battery_pct=87.5,\n    is_active=True,\n    reading_time=1739404800000000,\
      \  # epoch microseconds\n    tags=[\"outdoor\", \"roof-mount\"],\n)\nprint(record)\n\
      ```\n\n---\n\n## 5. Delta-to-Protobuf Type Mapping Reference\n\nHere is the\
      \ full mapping table. Your `.proto` must match the Delta table 1:1:\n\n| Delta\
      \ / Spark Type | Protobuf Type | Notes |\n|--------------------|---------------|-------|\n\
      | `STRING` | `string` | |\n| `INT` / `INTEGER` | `int32` | |\n| `LONG` / `BIGINT`\
      \ | `int64` | |\n| `FLOAT` | `float` | |\n| `DOUBLE` | `double` | |\n| `BOOLEAN`\
      \ | `bool` | |\n| `BINARY` | `bytes` | |\n| `ARRAY<T>` | `repeated T` | Element\
      \ type maps recursively |\n| `MAP<K,V>` | `map<K,V>` | Key must be string or\
      \ integer type |\n| `STRUCT` | Nested `message` | Fields map recursively |\n\
      | `DATE` | `int32` | Epoch days (days since 1970-01-01) |\n| `TIMESTAMP` | `int64`\
      \ | Epoch microseconds |\n| `DECIMAL(p,s)` | `bytes` or `string` | Check generated\
      \ `.proto` for exact mapping |\n| `VARIANT` | `string` | JSON-encoded string\
      \ |\n\n**Constraints:** Maximum 2000 columns per proto schema. Maximum 10 MB\
      \ per individual message.\n\n---\n\n## 6. Full Python Example: Ingesting with\
      \ the Compiled Protobuf\n\nHere is a complete, working Python producer using\
      \ the compiled Protobuf with Zerobus:\n\n```python\nimport os\nimport time\n\
      import iot_sensor_reading_pb2\n\nfrom zerobus.sdk.sync import ZerobusSdk\nfrom\
      \ zerobus.sdk.shared import RecordType, StreamConfigurationOptions, TableProperties\n\
      \n# --- Configuration ---\nserver_endpoint = os.environ[\"ZEROBUS_SERVER_ENDPOINT\"\
      ]\nworkspace_url   = os.environ[\"DATABRICKS_WORKSPACE_URL\"]\ntable_name  \
      \    = os.environ[\"ZEROBUS_TABLE_NAME\"]  # \"catalog.schema.iot_sensor_readings\"\
      \nclient_id       = os.environ[\"DATABRICKS_CLIENT_ID\"]\nclient_secret   =\
      \ os.environ[\"DATABRICKS_CLIENT_SECRET\"]\n\n# --- Initialize SDK and stream\
      \ ---\nsdk = ZerobusSdk(server_endpoint, workspace_url)\n\noptions = StreamConfigurationOptions(record_type=RecordType.PROTO)\n\
      table_props = TableProperties(\n    table_name,\n    iot_sensor_reading_pb2.IoTSensorReading.DESCRIPTOR,\n\
      )\n\nstream = sdk.create_stream(client_id, client_secret, table_props, options)\n\
      \n# --- Ingest demo records ---\ntry:\n    locations = [\"Building 7 Roof\"\
      , \"Warehouse A\", \"Parking Deck\", \"Server Room B2\"]\n    for i in range(20):\n\
      \        record = iot_sensor_reading_pb2.IoTSensorReading(\n            device_id=f\"\
      dev-{i:04d}\",\n            device_name=f\"sensor-{i}\",\n            location=locations[i\
      \ % len(locations)],\n            temperature=20.0 + (i * 0.3),\n          \
      \  humidity=50 + (i % 15),\n            pressure=1013.25 + (i * 0.1),\n    \
      \        battery_pct=100.0 - (i * 1.5),\n            is_active=True,\n     \
      \       reading_time=int(time.time() * 1_000_000),  # epoch microseconds\n \
      \           tags=[\"demo\", f\"batch-{i // 5}\"],\n        )\n        ack =\
      \ stream.ingest_record(record)\n        ack.wait_for_ack()\n        print(f\"\
      Record {i} ingested and durably acknowledged.\")\nfinally:\n    stream.close()\n\
      \nprint(\"All records ingested successfully.\")\n```\n\n**Key points in this\
      \ example:**\n\n- `RecordType.PROTO` tells the SDK to use Protobuf serialization.\n\
      - The `DESCRIPTOR` from the compiled `_pb2` module is passed to `TableProperties`\
      \ so the SDK knows the wire format.\n- `reading_time` is set as **epoch microseconds**\
      \ using `int(time.time() * 1_000_000)` -- this is required for TIMESTAMP columns.\n\
      - `ack.wait_for_ack()` blocks until the record is durably written. For higher\
      \ throughput, you can skip per-record ACK blocking and call `stream.flush()`\
      \ at the end instead.\n- The stream is always closed in a `finally` block to\
      \ release resources.\n\n---\n\n## Production Enhancement: Using the Reusable\
      \ Client Class\n\nFor production workloads, wrap the above in the `ZerobusClient`\
      \ class for automatic retry and reconnection:\n\n```python\nimport os\nimport\
      \ time\nimport iot_sensor_reading_pb2\n\nfrom zerobus.sdk.shared import RecordType\n\
      from zerobus_client import ZerobusClient  # the reusable client class\n\nwith\
      \ ZerobusClient(\n    server_endpoint=os.environ[\"ZEROBUS_SERVER_ENDPOINT\"\
      ],\n    workspace_url=os.environ[\"DATABRICKS_WORKSPACE_URL\"],\n    table_name=os.environ[\"\
      ZEROBUS_TABLE_NAME\"],\n    client_id=os.environ[\"DATABRICKS_CLIENT_ID\"],\n\
      \    client_secret=os.environ[\"DATABRICKS_CLIENT_SECRET\"],\n    record_type=RecordType.PROTO,\n\
      \    proto_descriptor=iot_sensor_reading_pb2.IoTSensorReading.DESCRIPTOR,\n\
      ) as client:\n    for i in range(100):\n        record = iot_sensor_reading_pb2.IoTSensorReading(\n\
      \            device_id=f\"dev-{i:04d}\",\n            device_name=f\"sensor-{i}\"\
      ,\n            location=\"Demo Site\",\n            temperature=22.0 + (i *\
      \ 0.1),\n            humidity=55 + (i % 10),\n            pressure=1013.25,\n\
      \            battery_pct=95.0,\n            is_active=True,\n            reading_time=int(time.time()\
      \ * 1_000_000),\n            tags=[\"production\", \"demo\"],\n        )\n \
      \       success = client.ingest(record)\n        if not success:\n         \
      \   print(f\"Failed to ingest record {i} after retries.\")\n```\n\nThis gives\
      \ you exponential backoff retries (1s, 2s, 4s) and automatic stream reconnection\
      \ on connection failures, plus clean resource management via the context manager.\n\
      \n---\n\n## Schema Evolution Reminder\n\nIf you later add columns to the Delta\
      \ table (e.g., `ALTER TABLE ... ADD COLUMNS ...`), you must:\n\n1. Regenerate\
      \ the `.proto` file (or update it manually)\n2. Recompile with `protoc`\n3.\
      \ Update your producer code to populate the new fields\n4. Redeploy\n\nZerobus\
      \ does not support automatic schema evolution -- the Protobuf schema and Delta\
      \ table must always match exactly."
    execution_success: false
  expectations: {}
  metadata:
    category: happy_path
    difficulty: medium
    source: grp
    approved_by: derek.witt
    approved_at: '2026-02-12T15:36:00.791750'
    skill_version: null
    fixed_by_commit: null
    fix_description: null
    trace_run_id: null
